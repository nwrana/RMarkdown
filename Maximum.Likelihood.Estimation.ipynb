{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression\n",
    "- https://www.cscu.cornell.edu/news/statnews/stnews50.pdf\n",
    "- http://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms\n",
    "\n",
    "#### Overview\n",
    "\n",
    "MLE is a method of estimating the parameters of a statistical model based on a set of observations. It is important to remember that:\n",
    "- a statistical model assumes that a particular observation (i.e. $x_{1},y_{1}$) approximates some type of probability distribution. Broadly speaking, a probabilty distribution outputs, as the name suggests, a probability: \n",
    "    - In Logistic Regression, it can be assuemd each data point follows the Sigmoid function\n",
    "    - In Linear Regression, it can be assumed each data point follows the Gaussian function \n",
    "\n",
    "In Machine Learning, the MLE is considered the cost function. It is the metric used to determine the quality of the model relative to a set of observations (i.e. training data).  \n",
    "\n",
    "##### Probability Distributions: A Quick Review\n",
    "\n",
    "Example: You work in a potato chip manufacturing company. You want to determine how much variation there is in a 500 g bag of potato chips. Over the years, you are able to sample the weight of 1 million bags. You plot each measurement on a frequency chart (histogram) and realize this produced a $\\textbf{normal distribution}$ curve.\n",
    "- this should be expected since these observations follow the $\\textit{Central Limit Theorem (CLT)}$. The mean (center of the curve) represents the expected value, or the most likely potato chip bag weight you should see (highest probability)\n",
    "- some bags will be a lot heavier than the mean, while others will be lighter. However, the probabilty of observing these weights will be much lower\n",
    "- the total range in weights from all observations (i.e. lowest weight observed to highest weight) is the variance for this distribution\n",
    "- once this distribution curve has been created (with mean $\\mu$ and variance $\\sigma^{2}$ unique to that distribution), one can now determine the chances of manufacturing a bag chips of a certain weight by calculating the $\\textbf{z-score}$ statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE in Detail\n",
    "\n",
    "We had mentioned above that the MLE is the cost function in a Machine Learnign problem. Therefore, the goal of MLE is to find th eparamters of a particular statistical model that is the best fit, or most likely to fit, the observed data.\n",
    "\n",
    "#### Analogy\n",
    "\n",
    "You have been summed to be part of a jury. You are presented with the following information:\n",
    "- the charges that describe the purpose of the trial\n",
    "- the truth of what happened according to the prosecution\n",
    "- the truth of what happened according to the defence\n",
    "- the evidence\n",
    "Goal: you the juror, within th econtext of the specified charges, and given the evidence, must select the version of the truth that most likely occured\n",
    "\n",
    "In MLE:\n",
    "- the framework of the trial is the statistical model selected by the user\n",
    "- the prosecution is one set of parameter values used to fit the statistical model\n",
    "- the defence is another set of parameter values used to fit the statistical model\n",
    "- the evidence are one set of observations, i.e. training data\n",
    "Goal: In reality there are many more 'versions of the truth' than just two (prosecution and defence). However the goal is the same, which is to find the parameter values that best fit the statistical model to the training data\n",
    "\n",
    "#### How does MLE do this ?\n",
    "\n",
    "- given a Supervised Learning training set, every value of x (i.e. the feature, predictor, independent variable) will have some observed ouput y (i.e. response, dependent variable). Your task is to model the relationship between x and y. i.e. estimate output y given x and parameters $\\theta$\n",
    "- the goal of MLE is to calculate how likely it would be to estimate the real response, y, using our current model and current model parameters\n",
    "- to calculate the $\\textit{Likelihood}$, we assume that each and every observation is $\\textbf{independent}$. This means that the likelihoos of seeing y for every x parameterized by theta are $\\textbf{multiplied}$. Intuitively, this is no different than calculating the probability of rolling a 4 twice in a row using a die (1/6 * 1/6 = 1/36)\n",
    "\n",
    "<blockquote>\n",
    "<strong>Note: Probability vs Likelihood</strong>\n",
    "<p>Probability is used before data are available to describe possible future outcomes given a fixed value for the parameter (or parameter vector). Likelihood is used after data are available to describe a function of a parameter (or parameter vector) for a given outcome.</p>\n",
    "</blockquote>\n",
    "\n",
    "- Therefore, we can represent the Likelihood function for a particular model with continuous probability distribution as: \n",
    "\n",
    "$$\\mathcal{L}(\\theta|x)=f_{\\theta}(x)$$\n",
    "\n",
    "- The MLE selects the set of values of the model parameters that maximizes the likelihood function. For given set of model paramaters, MLE is calculated by taking the dot product of each likelihood function at a given observation, $x$:\n",
    "\n",
    "$$\\mathcal{L}(\\theta;x_{1},...,x_{n})=\\prod_{i=1}^{n} f(x_{1},...x_{n}|\\theta)=f(x_{i}|\\theta)$$\n",
    "\n",
    "- In practice the algebra is more convenient to work with the natural logarithm of the likelihood function. This becomes clear during optimization. Taking the derivative of a dot product requires use of the product rule, and is exceedingly complicated as the number of observations and features increases (does not scale). Remember, we find the partial derivative of the cost function in order to implement the gradient ascent/descent algorithm in order to calculate the gradient. Therefore, taking the og of the Likelihood functino reduces the complexity of the equation:\n",
    "\n",
    "$$ln\\mathcal{L}(\\theta;x_{1},...,x_{n})=\\sum_{i=1}^{n} f(x_{i}|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00497680406472\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#Calculate the weighted linear relationhip\n",
    "#h(x)=weight_1*x_1 + weight_2*x_2\n",
    "weight= np.array([1,-1.5])\n",
    "x_1 = np.array([2,0,3,4])\n",
    "x_2 = np.array([1,2,3,1])\n",
    "\n",
    "features = np.column_stack([x_1,x_2])\n",
    "score = np.dot(features, weight)\n",
    "\n",
    "#actual observations. Need to separate positive outcomes from negative ones\n",
    "y = np.array([1,0,0,1])\n",
    "pos_index = np.array([0,3]) \n",
    "neg_index = np.array([1,2]) #remember, probability of a negative output is the compliment of a positive output\n",
    "\n",
    "#Calculate probabilities using the Logistic Model\n",
    "pos_logit = 1/(1+math.e**(-score[pos_index])) #probability that output is positive (when actual observtion is positive)\n",
    "neg_logit = 1/(1+math.e**(-score[neg_index])) #probabilty that output is positive (when actual observation is negative)\n",
    "\n",
    "#calculate Likelihood: the dot product of the probability of each observation\n",
    "likelihood = pos_logit[0]*pos_logit[1]*neg_logit[0]*neg_logit[1]\n",
    "\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLE in Linear Regression\n",
    "\n",
    "In Linear Regression, the most common way to fit our model to the observed data is the Residual Sum of Squares (RSS). This way of estimating parameters is very intuitive, however it's not the onyl way. We can also approach the same problem from a statistical framework, using MLE to accomplish the same thing.\n",
    "\n",
    "The linear model:\n",
    "$$h_{\\theta}(x)=\\theta x + \\epsilon$$\n",
    "\n",
    "where, $h_{\\theta}(x)$ is the estimated value, $\\theta$ is the parameter of the model, $x$ is the feature, and $\\epsilon$ is the error. Note that we did not include an intercept. This simplifies the problem without affecting the results.\n",
    "\n",
    "We can assume that the error is normally distributed with a mean centered at 0. This means that the probability distribution of the error term follows the Gaussian function. When the statistic $\\mu=0$, there is an equal chance that the error of the estimated value $h_{\\theta}(x)$ will be larger or smaller than the real observation $y$. The implications of this proof means that the term $\\theta x$ is also normally distributed:\n",
    "\n",
    "$$\\epsilon=h_{\\theta}(x) - \\theta x$$\n",
    "\n",
    "What this means:\n",
    "- the linear regression model ouputs a continuous variable (e.g. house price)\n",
    "- to use MLE, we need an output that's a probability distribution\n",
    "- at each value x in the dataset, the point that lies on the linear model is considered the mean value for a distribution with a constant variance. This also means that each point on that line has a gaussian distribution, where the degree in which an observation deviates from that mean is a probability contrained to that distribution\n",
    "- our hypothesis function is no longer the equation for a line as we saw in the standard Linear Regression problem using RSS, but the density curve of the normal distribution, with parameters $\\mu$ and $\\sigma$\n",
    "- the intuition behind this is that for every single $x$ and constant parameter $\\sigma$, the gaussian model will have an expected value, $\\mu$ at $h(x)$. What is the likelihood of seeing the observed $y$ ?\n",
    "- As before, we repeat this for every single $x$ in our dataset using the same parameters, multiply the probabilities, and come up with a score\n",
    "- this process cwill be repeated using the gradient ascent algorithm (incremental steps in parameter values) until the Maximum Likelihood Estimation is achieved\n",
    "- Note that this result will return parameter values almost identical to RSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLE in Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
